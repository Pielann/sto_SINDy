---
title: "S610-Final_Project-sSINDy"
author: "ChunHsien Lu"
date: "12/11/2019"
output: pdf_document
---

```{r setup, include=FALSE,echo=FALSE}

knitr::opts_chunk$set(echo = TRUE)

# --- functions --- #

#' @param data_X (numeric) data of X
#' @param data_y (numeric) data of y
#' @return (numeric) How many time a variable is chosen

order_coef=function(data_X,data_y){
  n=ncol(data_X)
  order_col=rep(n,n)
  coef_dot=rep(TRUE,n)
  Xi=rep(0,n)
  
  for(i in 1:(n-1)){
    Xi[coef_dot]= MASS::ginv(data_X[,coef_dot],0) %*% data_y
    order_col[coef_dot][which.min(abs(Xi[coef_dot]))]=i
    coef_dot[coef_dot][which.min(abs(Xi[coef_dot]))]=FALSE
  }
  return(order_col)
}


#' @param data_X (numeric) data of X
#' @param data_y (numeric) data of y
#' @param cv_group (numeric) cv_group
#' @param k_fold (numeric) k_fold cv
#' @return (numeric) errors

error_compute=function(data_X,data_y,cv_group,k_fold){
  sum_error=0
  if(ncol(data_X%>%as.matrix)>1){
    for (i in 1:k_fold) {
      data_X_train=data_X[cv_group==i,]
      data_X_test=data_X[!cv_group==i,]
      data_y_train=data_y[cv_group==i]
      data_y_test=data_y[!cv_group==i]
      Xi= MASS::ginv(data_X_train,0) %*% data_y_train
      sum_error=sum_error+sum((data_y_test-data_X_test%*%Xi)^2)
    }
  }
  else{
    for (i in 1:k_fold) {
      data_X_train=data_X[cv_group==i]
      data_X_test=data_X[!cv_group==i]
      data_y_train=data_y[cv_group==i]
      data_y_test=data_y[!cv_group==i]
      if(cov(data_X_train,data_X_train)==0){
        sum_error=sum_error+sum((data_y_test-mean(data_y_train))^2)
      }
      else{
        Xi= cov(data_X_train,data_y_train)/cov(data_X_train,data_X_train)
        sum_error=sum_error+sum((data_y_test-data_X_test*Xi)^2)
      }
    }
  }
  return(sum_error)
}




#' @param data_X (numeric) data of X
#' @param data_y (numeric) data of y
#' @param k_fold (numeric) k_fold cv
#' @return (list) list of some results

sSINDy=function(data_X,data_y,k_fold){
  library(magrittr)
  n_row=nrow(data_X)
  n_col=ncol(data_X)
  cv_group=sample(rep(1:k_fold,ceiling(n_row/k_fold)),size=n_row)
  errors=rep(0,n_col)
  coef_order=order_coef(data_X,data_y)
  coef_final=rep(0,n_col)
  
  
  errors=sapply((1:n_col),function(i)
    {error_compute(data_X[,coef_order>=(n_col-i+1)],data_y,cv_group,k_fold)})
  
  min_coef=which.min(errors)
  coef_final[coef_order>(n_col-min_coef)]=
    lm(data_y~data_X[,coef_order>(n_col-min_coef)]-1)%>%coef()
  
  return(list("coef_order"=coef_order,"cv_error"=errors,
              "min#"=min_coef,"coef"=coef_final))
}

#' @param data_X (numeric) data of X
#' @param data_y (numeric) data of y
#' @param k_fold (numeric) k_fold cv
#' @return (list) list of some results

sSINDy_new=function(data_X,data_y,k_fold){
  library(magrittr)
  
  n_row=nrow(data_X)
  n_col=ncol(data_X)
  cv_group=sample(rep(1:k_fold,ceiling(n_row/k_fold)),n_row)
  errors=rep(0,n_col+1)
  coef_order=order_coef(data_X,data_y)
  coef_final=rep(0,n_col)
  
  errors[2:(n_col+1)]=sapply((1:n_col),function(i)
  {error_compute(data_X[,coef_order>=(n_col-i+1)],data_y,cv_group,k_fold)})
  errors[1]=sum(data_y^2)*(k_fold-1)
    
  min_coef=which.min(errors)
  if(min_coef==1){
    coef_final=rep(0,n_col)
  }
  else{
  coef_final[coef_order>(n_col-min_coef+1)]=
    lm(data_y~data_X[,coef_order>(n_col-min_coef+1)]-1)%>%coef()
  }
  
  return(list("coef_order"=coef_order,"cv_error"=errors,
              "min#"=min_coef-1,"coef"=coef_final))
}



#' @param X (numeric) data of X
#' @param n_order (numeric) order of expansion
#' @return (numeric) expansion matrix

# Generate the data upto nth order
gene_poly_data=function(X,n_order){
  order_X=function(i){X^i}
  return(sapply((0:n_order), order_X))
}

```

\section{Introduction}

In this project, we mainly follow the paper written by Boninsegna et. al. This paper focuses on the reconstruction of an SDE. It does well on the example they provide. In order to understand the ideas provide in the paper, here, we attempt to apply the same method to Brownian motions(BMs). 

Suppose $X_t$ is an Ito's stochastic process, i.e.
$$dX_t=b(X_t)dt+\sigma(X_t)dB_t$$
where $b$ and $\sigma$ are functions, $B_t$ is a BM. The following is two important properties that can help us to reconstruct $b$ and $\sigma$:
$$b(x)=\lim_{s\rightarrow 0}\frac{1}{s} E[X_{t+s}-X_t|X_t=x]$$
and
$$\sigma(x)=\lim_{s\rightarrow 0}\frac{1}{s} E[(X_{t+s}-X_t)(X_{t+s}-X_t)|X_t=x].$$
Since what we want to do is BM, $b(x)=0$ and $\sigma(x)=1$. From the two properties, fixed some small $h>0$,we try to fit $b(x)$ by using $[X_{(m+1)h}-X_{mh}]_m$ and $[X_{mh}]_m$. Furthermore, $\sigma(x)$ is via $[(X_{(m+1)h}-X_{mh})(X_{(m+1)h}-X_{mh})]_m$ and $[X_{mh}]_m$. 

Intuitively, the algorithm proposed in the paper is just by using backward elimination and cross validation. The design matrix is $[M]_{ij}=[f_j(X_{ih})]_{ij}$ where $f_j$'s are possible candidates in $b(x)$. Assume $n$ functions $f_j$ are chosen. The full model is fitted by 
$$(X_{(i+1)h}-X_{ih})\sim \sum_{j=1}^n \beta_jf_j(X_{ih}).$$
The smaller model with $k-1$ predicted variables is removed the one with lowest absolute value in the model with $k$ variables. Via this way, we can have $n+1$ possible models.($y=0$ is included) Now, CV comes into play to decide the best model. 

The codes can be found in the following link: \newline
https://github.com/Pielann/sto_SINDy.git

\section{Simulation Design}
\subsection{Fit the drift term and explain the output of sSINDy:}

```{r,echo=FALSE,figure_height=3.5}
set.seed(123)
# Try to fit Brownian motion which start at x0=10
end_time=1 #The end time of the Brownian motion
n_times=1000 # How many time step b/t [0,end_time]

X=sde::BM(x=10, t0=0, T=end_time, N=n_times) # Generate BM
t_diff=end_time/n_times # Length of time interval
y_data_diff=(X[2:(n_times+1)]-X[1:n_times])
y_data=y_data_diff/t_diff # Change of BM b/t time
X_data=X[1:n_times] 

# Some facts for y_data
#This plot should be similar to normal N(0,var=t)
ggplot2::qplot(y_data_diff)

qqnorm(y_data_diff)
qqline(y_data_diff)

cat(" mean:",mean(y_data_diff),"\n",
    "variance:",var(y_data_diff))


```
Since $X_t$ is a BM, then $[X_{(m+1)h}-X_{mh}]_m$ should follow $N(0,h)$. We choose $h=0.001$ here. As shown above, the variance is 0.0009835, which is really closed to $h$.


```{r, echo=FALSE}
#############################################################
## Model fitting
n_fold=10 # folds of CV

# Fit drift part
# The real model is b(X)=0
sSINDy_new(gene_poly_data(X_data,3),y_data,10)

```
We did it by 10-fold cross-validation(CV) to compare the models which are up to 3rd order. More precisely, we can compare the models 
$$y=0,~y=\beta_{i_1}x^{i_1},~y=\beta_{i_1}x^{i_1}+\beta_{i_2}x^{i_2},~y=\beta_{i_1}x^{i_1}+\beta_{i_2}x^{i_2}+\beta_{i_3}x^{i_3},~y=\beta_{i_1}x^{i_1}+\beta_{i_2}x^{i_2}+\beta_{i_3}x^{i_3}+\beta_{i_4}x^{i_4},$$ 
wehre $i_1,i_2,i_3,i_4\in\{0,1,2,3\}$ are distinct. The output "coef_order" express the order of the coefficients to be removed. $1$ means the first variable to be removed. Hence, the model with three variables is
$$y=\beta_{0}+\beta_{1}x+\beta_{2}x^{2}$$
since the fouth term is the first one to be thrown. "cv_error" denotes the total errors via CV for models with variables from $0$ to $3$. "min#" means the model with least errors. Moreover, the "coef" is the coefficients from the best choice. Surprisingly, the true function for drift term is $0$. It shows that, in this example, we do really rebuild the drift term. 


\subsection{Fit the diffussion term:}
```{r,echo=FALSE}
#Fit diffsion term
variation=(X[2:(n_times+1)]-X[1:n_times])*(X[2:(n_times+1)]-X[1:n_times])/t_diff
sSINDy_new(gene_poly_data(X_data,3),variation,10)
```
The true diffusion term is $1.011784$ which is also really closed to $1$. Hence, the diffusion is rebuilt.

\section{Confidence Intervals}
\subsection{CI for Drift Terms}
```{r,echo=FALSE}
simulate_BM_drift=function(fun_sim,end_T,n_times,k_fold,order_poly=3){
  t_diff=end_T/n_times
  X=fun_sim(x=10, t0=0, end_T, n_times)
  y_data=(X[2:(n_times+1)]-X[1:n_times])
  X_data=X[1:n_times]
  return(sSINDy_new(gene_poly_data(X_data,order_poly),
                    y_data,k_fold)$coef)
}

coef_sim=replicate(1000,simulate_BM_drift(sde::BM,1,100,10))
apply(coef_sim,1,quantile,prob=c(0.025,0.975))
```
This simulation shows that the 95% CI are 0 for 1000 samples which has really good performance.


\subsection{CI for Diffussion Terms}
```{r,echo=FALSE}
# Simuation to check if we can reconstruct the diffusion term
simulate_BM_diffusion=function(fun_sim,end_T,n_times,k_fold,order_poly=3){
  t_diff=end_T/n_times
  X=fun_sim(x=10, t0=0, end_T, n_times)
  variation=(X[2:(n_times+1)]-X[1:n_times])*(X[2:(n_times+1)]-X[1:n_times])/t_diff
  X_data=X[1:n_times]
  return(sSINDy_new(gene_poly_data(X_data,order_poly),
                    variation,k_fold)$coef)
}

coef_sim=replicate(1000,simulate_BM_diffusion(sde::BM,1,1000,10))
apply(coef_sim,1,quantile,prob=c(0.025,0.975))

```
The diffusion term for standard BM is 1. However, even the true values are within the CI, the CI for the constant term is wide. This tells us that the variance is harder to reconstruct. 

\section{References}
Boninsegna et. al, Sparse learning of stochastic dynamical
equations, J. Chem. Phys. 148, 241723 (2018).
